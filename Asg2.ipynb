{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# (Optional) For reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "# Path to the cleaned dataset\n",
    "FILE_PATH = \"C:\\\\Users\\\\bramo\\\\Python Projects\\\\CAP4770-A2\\\\cleaned_restaurant_orders.csv\"\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(FILE_PATH)\n",
    "\n",
    "# Ensure dataset contains required columns\n",
    "expected_columns = [\"BillAmount\", \"Spending_per_Guest\", \"WaitTime\", \"Tip\"]\n",
    "missing_columns = [col for col in expected_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    raise KeyError(f\"⚠️ ERROR: Missing columns in dataset: {missing_columns}\")\n",
    "else:\n",
    "    print(\"✅ Dataset loaded successfully. All expected columns are present.\")\n",
    "\n",
    "# Set Matplotlib style\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "##############################\n",
    "#          Utilities\n",
    "##############################\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    \"\"\"Load CSV dataset into a pandas DataFrame.\"\"\"\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "# Ensure df is numeric before applying heatmap\n",
    "df_numeric = df.select_dtypes(include=['number'])\n",
    "\n",
    "# Verify df_numeric is not empty\n",
    "if not df_numeric.empty:\n",
    "    # Feature Correlation Heatmap\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(df_numeric.corr(), annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "    plt.title(\"Feature Correlation Heatmap\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️ No numeric features found for correlation matrix.\")\n",
    "\n",
    "# Scatter Plots for Key Relationships (Tip vs. Key Features)\n",
    "selected_features = [\"BillAmount\", \"Spending_per_Guest\", \"WaitTime\"]\n",
    "if \"Tip\" in df.columns:\n",
    "    sns.pairplot(df, x_vars=selected_features, y_vars=[\"Tip\"], kind=\"scatter\")\n",
    "    plt.suptitle(\"Scatter Plots of Features vs. Tip\", y=1.02)\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"⚠️ 'Tip' column not found. Skipping scatter plot.\")\n",
    "\n",
    "def preprocess_data(df, feature_cols, target_col, normalize=True):\n",
    "    \"\"\"\n",
    "    Extract features and target, convert to tensors, and optionally normalize.\n",
    "    Returns:\n",
    "      - X_tensor\n",
    "      - y_tensor\n",
    "      - X_mean, X_std (or None, None if normalize=False)\n",
    "      - y_mean, y_std (or None, None if normalize=False)\n",
    "    \"\"\"\n",
    "    if target_col not in df.columns:\n",
    "        raise KeyError(f\"⚠️ ERROR: Column '{target_col}' not found in dataset!\")\n",
    "\n",
    "    X = df[feature_cols].values\n",
    "    y = df[[target_col]].values  # keep target as column vector\n",
    "    \n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "    \n",
    "    if normalize:\n",
    "        X_mean, X_std = X_tensor.mean(dim=0), X_tensor.std(dim=0)\n",
    "        # Guard against std=0\n",
    "        X_std[X_std == 0] = 1.0\n",
    "        \n",
    "        y_mean, y_std = y_tensor.mean(), y_tensor.std()\n",
    "        if y_std == 0:\n",
    "            y_std = 1.0\n",
    "        \n",
    "        X_tensor = (X_tensor - X_mean) / X_std\n",
    "        y_tensor = (y_tensor - y_mean) / y_std\n",
    "        \n",
    "        return X_tensor, y_tensor, X_mean, X_std, y_mean, y_std\n",
    "    else:\n",
    "        return X_tensor, y_tensor, None, None, None, None\n",
    "\n",
    "def plot_regression_line(\n",
    "    X_orig, y_orig, \n",
    "    X_tensor, model, \n",
    "    X_mean, X_std, y_mean, y_std,\n",
    "    title=\"Regression Plot\", \n",
    "    xlabel=\"Feature\", \n",
    "    ylabel=\"Target\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the original data and the best-fit regression line.\n",
    "    Converts normalized predictions back to original scale.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predicted = model(X_tensor).detach().flatten()\n",
    "    \n",
    "    # Convert predictions back to original scale if normalization was used\n",
    "    y_pred_orig = predicted * y_std + y_mean\n",
    "    \n",
    "    # Sort for a smooth line\n",
    "    sorted_indices = X_orig.argsort()\n",
    "    X_sorted = X_orig[sorted_indices]\n",
    "    y_pred_sorted = y_pred_orig.numpy()[sorted_indices]\n",
    "\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.scatter(X_orig, y_orig, alpha=0.5, label=\"Data Points\")\n",
    "    plt.plot(X_sorted, y_pred_sorted, color='red', linewidth=2, label=\"Regression Line\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def polynomial_features(X, degree):\n",
    "    \"\"\"\n",
    "    Given a tensor X of shape (n_samples, n_features), return a new tensor \n",
    "    with each feature raised to powers 1 through 'degree' (no cross-terms). \n",
    "    Output shape: (n_samples, n_features * degree).\n",
    "    \"\"\"\n",
    "    poly_terms = [X ** d for d in range(1, degree + 1)]\n",
    "    return torch.cat(poly_terms, dim=1)\n",
    "\n",
    "\n",
    "##############################\n",
    "#       Models & Training\n",
    "##############################\n",
    "\n",
    "class LinearRegressionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Linear Regression Model that can handle \n",
    "    one or multiple input features.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=1):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def train_model(\n",
    "    model, \n",
    "    X, \n",
    "    y, \n",
    "    learning_rate=0.001, \n",
    "    epochs=1000, \n",
    "    print_every=100,\n",
    "    weight_decay=0.0, \n",
    "    early_stopping_patience=None,\n",
    "    optimizer=None  \n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model using MSE loss.\n",
    "    - Supports Adam, SGD, or any optimizer.\n",
    "    - Uses optional early stopping.\n",
    "    \"\"\"\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    if optimizer is None:\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=100, factor=0.5, verbose=False\n",
    "    )\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    loss_history = []  \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(X)\n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        if early_stopping_patience is not None:\n",
    "            if loss.item() < best_loss:\n",
    "                best_loss = loss.item()\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(f\"[Early Stopping] Epoch={epoch}, Loss={loss.item():.4f}\")\n",
    "                break\n",
    "        \n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            current_lr = scheduler.optimizer.param_groups[0]['lr']\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.6f}, LR: {current_lr:.6f}\")\n",
    "\n",
    "    return model, loss_history  # ✅ Now returning both the trained model and loss values\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Single-Feature Linear Regression\n",
    "\n",
    "In this part, we will pick \"BillAmount\" as our single feature to predict \"Tip\". \n",
    "We will demonstrate the training process, final parameters, final loss, \n",
    "and plot the regression line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1 - Single Feature Regression (Comparing Adam vs. SGD)\n",
    "\n",
    "# 1) Load the dataset\n",
    "df = load_dataset(FILE_PATH)\n",
    "\n",
    "# 2) Define single feature and target\n",
    "feature_cols = [\"BillAmount\"]\n",
    "target_col = \"Tip\"\n",
    "\n",
    "# Ensure 'Tip' column exists before proceeding\n",
    "if target_col not in df.columns:\n",
    "    raise KeyError(f\"⚠️ ERROR: '{target_col}' column is missing from dataset!\")\n",
    "\n",
    "# 3) Preprocess data (normalize=True)\n",
    "X_tensor, y_tensor, X_mean, X_std, y_mean, y_std = preprocess_data(\n",
    "    df, feature_cols, target_col, normalize=True\n",
    ")\n",
    "\n",
    "# 4) Initialize Single-Feature Linear Regression Models\n",
    "model_adam = LinearRegressionModel(input_dim=1)\n",
    "model_sgd = LinearRegressionModel(input_dim=1)\n",
    "\n",
    "# 5) Define Optimizers\n",
    "optimizer_adam = optim.Adam(model_adam.parameters(), lr=0.001)\n",
    "optimizer_sgd = optim.SGD(model_sgd.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Train both models and store loss history\n",
    "model_adam, loss_adam = train_model(\n",
    "    model_adam,\n",
    "    X_tensor,\n",
    "    y_tensor,\n",
    "    learning_rate=0.001,\n",
    "    epochs=1000,\n",
    "    print_every=100,\n",
    "    weight_decay=0.0,\n",
    "    early_stopping_patience=None,\n",
    "    optimizer=optimizer_adam  # Now supported\n",
    ")\n",
    "\n",
    "model_sgd, loss_sgd = train_model(\n",
    "    model_sgd,\n",
    "    X_tensor,\n",
    "    y_tensor,\n",
    "    learning_rate=0.001,\n",
    "    epochs=1000,\n",
    "    print_every=100,\n",
    "    weight_decay=0.0,\n",
    "    early_stopping_patience=None,\n",
    "    optimizer=optimizer_sgd  # Now supported\n",
    ")\n",
    "\n",
    "\n",
    "# 7) Evaluate final loss (MSE) for both models\n",
    "with torch.no_grad():\n",
    "    y_pred_adam = model_adam(X_tensor)\n",
    "    mse_loss_adam = nn.MSELoss()(y_pred_adam, y_tensor)\n",
    "\n",
    "    y_pred_sgd = model_sgd(X_tensor)\n",
    "    mse_loss_sgd = nn.MSELoss()(y_pred_sgd, y_tensor)\n",
    "\n",
    "print(f\"✅ Final MSE (Adam, normalized): {mse_loss_adam.item():.4f}\")\n",
    "print(f\"✅ Final MSE (SGD, normalized): {mse_loss_sgd.item():.4f}\")\n",
    "\n",
    "# 8) Convert predictions to original scale and compute R²\n",
    "y_pred_actual_adam = y_pred_adam * y_std + y_mean\n",
    "y_pred_actual_sgd = y_pred_sgd * y_std + y_mean\n",
    "y_actual = y_tensor * y_std + y_mean\n",
    "\n",
    "ss_total = torch.sum((y_actual - torch.mean(y_actual))**2)\n",
    "\n",
    "ss_residual_adam = torch.sum((y_actual - y_pred_actual_adam)**2)\n",
    "r2_adam = 1 - (ss_residual_adam / ss_total)\n",
    "\n",
    "ss_residual_sgd = torch.sum((y_actual - y_pred_actual_sgd)**2)\n",
    "r2_sgd = 1 - (ss_residual_sgd / ss_total)\n",
    "\n",
    "print(f\"✅ R² Score (Adam, unnormalized): {r2_adam.item():.4f}\")\n",
    "print(f\"✅ R² Score (SGD, unnormalized): {r2_sgd.item():.4f}\")\n",
    "\n",
    "# 9) Plot Loss Curves to Compare Convergence Speed\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(loss_adam, label=\"Adam\", color=\"blue\")\n",
    "plt.plot(loss_sgd, label=\"SGD + Momentum\", color=\"red\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve: Adam vs. SGD\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 10) Plot regression lines for both models\n",
    "X_orig = df[feature_cols[0]].values\n",
    "y_orig = df[target_col].values\n",
    "\n",
    "plot_regression_line(\n",
    "    X_orig,\n",
    "    y_orig,\n",
    "    X_tensor,\n",
    "    model_adam,\n",
    "    X_mean,\n",
    "    X_std,\n",
    "    y_mean,\n",
    "    y_std,\n",
    "    title=\"Part 1: Single-Feature Regression (Adam)\",\n",
    "    xlabel=\"Bill Amount ($)\",\n",
    "    ylabel=\"Tip ($)\"\n",
    ")\n",
    "\n",
    "plot_regression_line(\n",
    "    X_orig,\n",
    "    y_orig,\n",
    "    X_tensor,\n",
    "    model_sgd,\n",
    "    X_mean,\n",
    "    X_std,\n",
    "    y_mean,\n",
    "    y_std,\n",
    "    title=\"Part 1: Single-Feature Regression (SGD + Momentum)\",\n",
    "    xlabel=\"Bill Amount ($)\",\n",
    "    ylabel=\"Tip ($)\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Multiple-Feature Linear Regression\n",
    "\n",
    "Here, we’ll use three features: BillAmount, NumItems and Spending_per_Guest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "#   Part 2 - Multi-Feature Regression (Optimized)\n",
    "##############################\n",
    "\n",
    "# 1) Load the dataset\n",
    "df = load_dataset(FILE_PATH)\n",
    "\n",
    "# 2) Specify the best three features based on correlation\n",
    "features = [\"BillAmount\", \"NumItems\", \"Tip_Percentage\"]  # ✅ Optimized features\n",
    "target_col = \"Tip\"\n",
    "\n",
    "# 3) Preprocess data (normalize=True)\n",
    "X_tensor, y_tensor, X_mean, X_std, y_mean, y_std = preprocess_data(\n",
    "    df, features, target_col, normalize=True\n",
    ")\n",
    "\n",
    "# Print feature scaling values to verify normalization\n",
    "print(f\"X Mean: {X_mean.numpy()}, X Std: {X_std.numpy()}\")\n",
    "print(f\"y Mean: {y_mean.numpy()}, y Std: {y_std.numpy()}\")\n",
    "\n",
    "# 4) Initialize the optimized multi-feature Linear Regression model\n",
    "model_multi = LinearRegressionModel(input_dim=len(features))\n",
    "\n",
    "# Print model type before training (debugging potential tuple error)\n",
    "print(\"Before Training, Model Type:\", type(model_multi))\n",
    "\n",
    "# 5) Train-Test Split (80% Training, 20% Validation)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_tensor, y_tensor, test_size=0.2, random_state=0)\n",
    "\n",
    "# 6) Train the model (Unpacking model and loss history correctly)\n",
    "model_multi, loss_multi = train_model(  # ✅ FIXED unpacking issue\n",
    "    model_multi,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    learning_rate=0.0003,  # ✅ Lower learning rate for stability\n",
    "    epochs=2000,  # ✅ Increased training time for better results\n",
    "    print_every=200,\n",
    "    weight_decay=0.001  # L2 Regularization to prevent overfitting\n",
    ")\n",
    "\n",
    "# Print model type after training (debugging potential tuple error)\n",
    "print(\"After Training, Model Type:\", type(model_multi))  # ✅ Should now be a model, NOT a tuple!\n",
    "\n",
    "# 7) Evaluate final MSE on normalized scale (Validation Set)\n",
    "with torch.no_grad():\n",
    "    y_pred_train = model_multi(X_train)\n",
    "    y_pred_val = model_multi(X_val)\n",
    "\n",
    "mse_loss_train = nn.MSELoss()(y_pred_train, y_train)\n",
    "mse_loss_val = nn.MSELoss()(y_pred_val, y_val)\n",
    "\n",
    "print(f\"✅ Final Training MSE: {mse_loss_train.item():.4f}\")\n",
    "print(f\"✅ Final Validation MSE: {mse_loss_val.item():.4f}\")  # ✅ Helps detect overfitting\n",
    "\n",
    "# 8) Convert predictions back to the original scale and compute R²\n",
    "y_pred_actual = y_pred_train * y_std + y_mean\n",
    "y_actual = y_train * y_std + y_mean\n",
    "\n",
    "ss_total = torch.sum((y_actual - torch.mean(y_actual))**2)\n",
    "ss_residual = torch.sum((y_actual - y_pred_actual) ** 2)  # ✅ Corrected squaring\n",
    "r2 = 1 - (ss_residual / ss_total)\n",
    "\n",
    "print(f\"✅ R² Score (unnormalized): {r2.item():.4f}\")  # ✅ Should be more reasonable now\n",
    "\n",
    "# 9) Print out learned weights and bias\n",
    "print(\"Learned Weights:\", model_multi.linear.weight.data.numpy())\n",
    "print(\"Learned Bias:\", model_multi.linear.bias.data.item())\n",
    "\n",
    "# 10) Plot Training Loss for Multi-Feature Regression\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(loss_multi, label=\"Multi-Feature Regression\", color=\"purple\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Loss Curve: Multi-Feature Regression\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 11) Plot Actual vs. Predicted Tip (unnormalized)\n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(x=y_actual.numpy().flatten(), y=y_pred_actual.numpy().flatten(), alpha=0.5, label=\"Data Points\")\n",
    "sns.regplot(\n",
    "    x=y_actual.numpy().flatten(),\n",
    "    y=y_pred_actual.numpy().flatten(),\n",
    "    scatter=False, \n",
    "    color='red', \n",
    "    label=\"Regression Fit\"\n",
    ")\n",
    "plt.xlabel(\"Actual Tip ($)\")\n",
    "plt.ylabel(\"Predicted Tip ($)\")\n",
    "plt.title(f\"Part 2: Multi-Feature Regression (Optimized), R² = {r2.item():.4f}\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Polynomial Regression\n",
    "\n",
    "We compare different polynomial degrees (2, 4, 6) using the same 3 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "#   Part 3 - Polynomial Regression\n",
    "##############################\n",
    "\n",
    "# 1) Load the dataset\n",
    "df = load_dataset(FILE_PATH)\n",
    "\n",
    "# 2) Define the best three features and the target\n",
    "features = [\"BillAmount\", \"NumItems\", \"Tip_Percentage\"]  # ✅ Updated feature selection\n",
    "target_col = \"Tip\"\n",
    "\n",
    "# 3) Preprocess data (normalize=True)\n",
    "X_tensor, y_tensor, X_mean, X_std, y_mean, y_std = preprocess_data(\n",
    "    df, features, target_col, normalize=True\n",
    ")\n",
    "\n",
    "# We will test 3 polynomial degrees\n",
    "degrees = [2, 4, 6]\n",
    "\n",
    "# Dictionary to store final MSE & R² results for each degree\n",
    "results = {}\n",
    "\n",
    "for degree in degrees:\n",
    "    print(f\"\\n=== Polynomial Degree {degree} ===\")\n",
    "    \n",
    "    # 4) Generate polynomial features (no cross-terms)\n",
    "    X_poly = polynomial_features(X_tensor, degree)\n",
    "    \n",
    "    # 5) Initialize and train a linear model on X_poly\n",
    "    model_poly = LinearRegressionModel(input_dim=X_poly.shape[1])\n",
    "    model_poly, loss_poly = train_model(  # ✅ FIXED unpacking issue\n",
    "        model_poly,\n",
    "        X_poly,\n",
    "        y_tensor,\n",
    "        learning_rate=0.001,\n",
    "        epochs=1000,\n",
    "        print_every=200,\n",
    "        weight_decay=0.01,   # ✅ Optional small L2 regularization to mitigate overfitting\n",
    "        early_stopping_patience=200  # ✅ Optional patience for early stopping\n",
    "    )\n",
    "    \n",
    "    # 6) Evaluate final MSE (on normalized scale)\n",
    "    with torch.no_grad():\n",
    "        y_pred = model_poly(X_poly)\n",
    "\n",
    "    mse_loss = nn.MSELoss()(y_pred, y_tensor)\n",
    "    print(f\"✅ Final MSE (normalized): {mse_loss.item():.4f}\")\n",
    "    \n",
    "    # Convert predictions back to original scale to compute R²\n",
    "    y_pred_actual = y_pred * y_std + y_mean\n",
    "    y_actual = y_tensor * y_std + y_mean\n",
    "    \n",
    "    ss_total = torch.sum((y_actual - torch.mean(y_actual))**2)\n",
    "    ss_residual = torch.sum((y_actual - y_pred_actual) ** 2)  # ✅ Corrected squaring issue\n",
    "    r2 = 1 - (ss_residual / ss_total)\n",
    "    print(f\"✅ R² Score (unnormalized): {r2.item():.4f}\")\n",
    "    \n",
    "    # Store in our results dict\n",
    "    results[degree] = {\n",
    "        \"MSE_normalized\": mse_loss.item(),\n",
    "        \"R2_unnormalized\": r2.item()\n",
    "    }\n",
    "    \n",
    "    # 7) Plot Actual vs. Predicted\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.scatterplot(\n",
    "        x=y_actual.numpy().flatten(), \n",
    "        y=y_pred_actual.numpy().flatten(),\n",
    "        alpha=0.5, label=\"Data Points\"\n",
    "    )\n",
    "    sns.regplot(\n",
    "        x=y_actual.numpy().flatten(),\n",
    "        y=y_pred_actual.numpy().flatten(),\n",
    "        scatter=False, color='red',\n",
    "        label=f\"Poly Deg={degree}\"\n",
    "    )\n",
    "    plt.xlabel(\"Actual Tip ($)\")\n",
    "    plt.ylabel(\"Predicted Tip ($)\")\n",
    "    plt.title(f\"Part 3: Polynomial Regression (Degree {degree})\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# 8) Print overall summary of results\n",
    "print(\"\\n=== Polynomial Regression Summary ===\")\n",
    "for deg, metrics in results.items():\n",
    "    print(f\"Degree={deg}: MSE (norm)={metrics['MSE_normalized']:.4f}, R²(unnorm)={metrics['R2_unnormalized']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Binary Classification\n",
    "\n",
    "We create a binary label (Satisfied vs. Unsatisfied) and train a logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 4 - Binary Classification (Comparing Logistic Regression vs. Neural Network)\n",
    "\n",
    "##############################\n",
    "# 1) Load & Prepare the Data\n",
    "##############################\n",
    "from sklearn.metrics import auc, precision_recall_fscore_support, roc_curve\n",
    "\n",
    "\n",
    "df = load_dataset(FILE_PATH)\n",
    "\n",
    "# Create a binary label: 1 for \"Satisfied\", 0 for anything else\n",
    "df[\"BinarySatisfaction\"] = df[\"CustomerSatisfaction\"].apply(lambda x: 1 if x == \"Satisfied\" else 0)\n",
    "\n",
    "# ✅ Features Used (Same for Both Models)\n",
    "features = [\"BillAmount\", \"WaitTime\", \"NumItems\", \"Spending_per_Guest\", \"Is_Weekend\", \"Hour\", \"Tip_Percentage\"]\n",
    "target_col = \"BinarySatisfaction\"\n",
    "\n",
    "# Convert categorical variables if needed\n",
    "if df[\"Is_Weekend\"].dtype == object:\n",
    "    df[\"Is_Weekend\"] = df[\"Is_Weekend\"].apply(lambda x: 1 if str(x).lower() in [\"yes\", \"true\"] else 0)\n",
    "\n",
    "# Convert \"DayOfWeek\" to numerical representation (Optional, if categorical)\n",
    "if \"DayOfWeek\" in df.columns and df[\"DayOfWeek\"].dtype == object:\n",
    "    df[\"DayOfWeek\"] = df[\"DayOfWeek\"].astype(\"category\").cat.codes\n",
    "\n",
    "# Train/test split (80/20)\n",
    "train_df = df.sample(frac=0.8, random_state=42)\n",
    "test_df  = df.drop(train_df.index)\n",
    "\n",
    "##############################\n",
    "# 2) Preprocess Features\n",
    "##############################\n",
    "# We'll normalize the features, but NOT the target\n",
    "X_train, _, X_mean, X_std, _, _ = preprocess_data(\n",
    "    train_df, \n",
    "    features, \n",
    "    target_col, \n",
    "    normalize=True\n",
    ")\n",
    "y_train = torch.tensor(train_df[target_col].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "X_test, _, _, _, _, _ = preprocess_data(\n",
    "    test_df, \n",
    "    features, \n",
    "    target_col, \n",
    "    normalize=True\n",
    ")\n",
    "y_test = torch.tensor(test_df[target_col].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "##############################\n",
    "# 3) Model 1: Logistic Regression (Allowed by Guidelines)\n",
    "##############################\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)  # ✅ Only One Linear Layer (Allowed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)  # ✅ No Activation, BCEWithLogitsLoss expects logits\n",
    "\n",
    "def train_model(\n",
    "    model, \n",
    "    X, \n",
    "    y, \n",
    "    learning_rate=0.001, \n",
    "    epochs=500, \n",
    "    print_every=50,\n",
    "    weight_decay=0.01  # ✅ L2 Regularization (Allowed)\n",
    "):\n",
    "    \"\"\"\n",
    "    Train a binary classifier using BCEWithLogitsLoss.\n",
    "    \"\"\"\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(X)                      \n",
    "        loss = criterion(logits, y)  # BCE loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "##############################\n",
    "# 4) Model 2: Neural Network (For Comparison)\n",
    "##############################\n",
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 8)  # Hidden layer with 8 neurons\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(8, 1)  # Output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "##############################\n",
    "# 5) Train Both Models & Compare\n",
    "##############################\n",
    "# Train Logistic Regression\n",
    "model_logistic = LogisticRegressionModel(input_dim=X_train.shape[1])\n",
    "model_logistic = train_model(\n",
    "    model_logistic, X_train, y_train,\n",
    "    learning_rate=0.001, epochs=500, print_every=50\n",
    ")\n",
    "\n",
    "# Train Neural Network\n",
    "model_mlp = MLPClassifier(input_dim=X_train.shape[1])\n",
    "model_mlp = train_model(\n",
    "    model_mlp, X_train, y_train,\n",
    "    learning_rate=0.001, epochs=500, print_every=50\n",
    ")\n",
    "\n",
    "##############################\n",
    "# 6) Evaluate Both Models\n",
    "##############################\n",
    "def evaluate_model(model, X, y, threshold=0.4):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(X)\n",
    "        probs = torch.sigmoid(logits)  \n",
    "\n",
    "    preds = (probs >= threshold).float()  # ✅ Adjusted threshold for better recall\n",
    "    accuracy = (preds.eq(y)).float().mean().item()\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y.numpy(), preds.numpy(), average=\"binary\"\n",
    "    )\n",
    "\n",
    "    return accuracy, precision, recall, f1, probs.numpy()\n",
    "\n",
    "# Evaluate Logistic Regression\n",
    "acc_log, prec_log, rec_log, f1_log, probs_log = evaluate_model(model_logistic, X_test, y_test)\n",
    "\n",
    "# Evaluate Neural Network\n",
    "acc_mlp, prec_mlp, rec_mlp, f1_mlp, probs_mlp = evaluate_model(model_mlp, X_test, y_test)\n",
    "\n",
    "# Print comparison results\n",
    "print(\"\\nModel Performance Comparison\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Logistic Regression - Accuracy: {acc_log:.4f}, Precision: {prec_log:.2f}, Recall: {rec_log:.2f}, F1-Score: {f1_log:.2f}\")\n",
    "print(f\"Neural Network      - Accuracy: {acc_mlp:.4f}, Precision: {prec_mlp:.2f}, Recall: {rec_mlp:.2f}, F1-Score: {f1_mlp:.2f}\")\n",
    "\n",
    "##############################\n",
    "# 7) Plot ROC Curves, Heatmap & Correlation Matrix\n",
    "##############################\n",
    "# Plot ROC Curves\n",
    "fpr_log, tpr_log, _ = roc_curve(y_test.numpy(), probs_log)\n",
    "roc_auc_log = auc(fpr_log, tpr_log)\n",
    "\n",
    "fpr_mlp, tpr_mlp, _ = roc_curve(y_test.numpy(), probs_mlp)\n",
    "roc_auc_mlp = auc(fpr_mlp, tpr_mlp)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(fpr_log, tpr_log, color=\"blue\", label=f\"Logistic Regression (AUC = {roc_auc_log:.2f})\")\n",
    "plt.plot(fpr_mlp, tpr_mlp, color=\"red\", label=f\"Neural Network (AUC = {roc_auc_mlp:.2f})\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\", color=\"gray\")  # Diagonal reference line\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve Comparison\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Compute correlation matrix (including target variable)\n",
    "df_numeric = df[features + [\"BinarySatisfaction\"]].select_dtypes(include=[\"number\"])\n",
    "corr_matrix = df_numeric.corr()\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Feature Correlation Heatmap (Including Binary Target)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Multiple-Class Classification\n",
    "\n",
    "We will create a 3-class label (e.g., 'Low Tip', 'Medium Tip', 'High Tip') or \n",
    "use 'CustomerSatisfaction' if it has 3 categories, and implement One-vs-All logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "#   Part 5 - Multi-Class Classification (OvA & OvO)\n",
    "##############################\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support\n",
    "from itertools import combinations\n",
    "from collections import Counter\n",
    "\n",
    "##############################\n",
    "# 1) Load & Prepare the Data\n",
    "##############################\n",
    "df = load_dataset(FILE_PATH)\n",
    "\n",
    "# Categorize tips into 3 classes: Low, Medium, High\n",
    "def categorize_tip(tip_value):\n",
    "    if tip_value < 2.0:\n",
    "        return 0  # \"Low Tip\"\n",
    "    elif tip_value <= 5.0:\n",
    "        return 1  # \"Medium Tip\"\n",
    "    else:\n",
    "        return 2  # \"High Tip\"\n",
    "\n",
    "df[\"TipCategory\"] = df[\"Tip\"].apply(categorize_tip)\n",
    "print(\"Unique classes in TipCategory:\", df[\"TipCategory\"].unique())\n",
    "\n",
    "# **Updated Feature Selection** (Replaced Spending_per_Guest with Tip_Percentage)\n",
    "feature_cols = [\"BillAmount\", \"WaitTime\", \"NumItems\", \"Tip_Percentage\", \"Is_Weekend\"]\n",
    "target_col = \"TipCategory\"\n",
    "\n",
    "# Ensure Is_Weekend is numeric\n",
    "if df[\"Is_Weekend\"].dtype == object:\n",
    "    df[\"Is_Weekend\"] = df[\"Is_Weekend\"].apply(lambda x: 1 if str(x).lower() in [\"yes\",\"true\"] else 0)\n",
    "\n",
    "# Train/Test Split (80/20)\n",
    "train_df = df.sample(frac=0.8, random_state=42)\n",
    "test_df = df.drop(train_df.index)\n",
    "\n",
    "# Balance the classes (Oversampling if needed)\n",
    "class_counts = Counter(train_df[target_col])\n",
    "print(\"Class Distribution Before Oversampling:\", class_counts)\n",
    "\n",
    "max_class_count = max(class_counts.values())\n",
    "balanced_train_df = train_df.copy()\n",
    "\n",
    "for class_label, count in class_counts.items():\n",
    "    if count < max_class_count:\n",
    "        additional_samples = train_df[train_df[target_col] == class_label].sample(\n",
    "            n=(max_class_count - count), replace=True, random_state=42\n",
    "        )\n",
    "        balanced_train_df = pd.concat([balanced_train_df, additional_samples])\n",
    "\n",
    "print(\"Class Distribution After Oversampling:\", Counter(balanced_train_df[target_col]))\n",
    "\n",
    "# Preprocess features (normalize)\n",
    "X_train, _, X_mean, X_std, _, _ = preprocess_data(balanced_train_df, feature_cols, target_col, normalize=True)\n",
    "y_train = torch.tensor(balanced_train_df[target_col].values, dtype=torch.long)  # Multi-class labels (long type)\n",
    "X_test, _, _, _, _, _ = preprocess_data(test_df, feature_cols, target_col, normalize=True)\n",
    "y_test = torch.tensor(test_df[target_col].values, dtype=torch.long)\n",
    "\n",
    "num_classes = len(df[\"TipCategory\"].unique())\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "##############################\n",
    "# 2) One-vs-All (OvA) Training\n",
    "##############################\n",
    "class LogisticRegressionModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)  # Outputs raw logits\n",
    "\n",
    "def train_logistic_model(model, X, y, lr=0.001, epochs=500, print_every=100):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        logits = model(X)\n",
    "        loss = criterion(logits, y.view(-1, 1).float())  # BCE requires float [N,1]\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (epoch + 1) % print_every == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def train_ova_classifiers(X, y, num_classes, lr=0.001, epochs=500):\n",
    "    models = []\n",
    "    for k in range(num_classes):\n",
    "        print(f\"\\n--- Training OvA Classifier for class={k} vs. others ---\")\n",
    "        y_k = (y == k).long()\n",
    "        model_k = LogisticRegressionModel(input_dim=X.shape[1])\n",
    "        model_k = train_logistic_model(model_k, X, y_k, lr=lr, epochs=epochs, print_every=100)\n",
    "        models.append(model_k)\n",
    "    return models\n",
    "\n",
    "def predict_ova(models, X):\n",
    "    model_logits = []\n",
    "    with torch.no_grad():\n",
    "        for mdl in models:\n",
    "            logit = mdl(X)\n",
    "            model_logits.append(logit)\n",
    "\n",
    "    logits_stacked = torch.cat(model_logits, dim=1)\n",
    "    preds = torch.argmax(logits_stacked, dim=1)\n",
    "    return preds\n",
    "\n",
    "print(\"\\n============= One-vs-All (OvA) =============\")\n",
    "ova_models = train_ova_classifiers(X_train, y_train, num_classes, lr=0.001, epochs=500)\n",
    "ova_preds_test = predict_ova(ova_models, X_test)\n",
    "ova_accuracy = (ova_preds_test == y_test).float().mean().item()\n",
    "print(f\"OvA Test Accuracy: {ova_accuracy:.4f}\")\n",
    "\n",
    "##############################\n",
    "# 3) One-vs-One (OvO) Training\n",
    "##############################\n",
    "def train_ovo_classifiers(X, y, num_classes, lr=0.001, epochs=500):\n",
    "    pairwise_models = {}\n",
    "    for (i, j) in combinations(range(num_classes), 2):\n",
    "        print(f\"\\n--- Training OvO Classifier for classes={i} vs. {j} ---\")\n",
    "        mask_ij = (y == i) | (y == j)\n",
    "        X_ij = X[mask_ij]\n",
    "        y_ij = y[mask_ij]\n",
    "        y_ij_binary = (y_ij == j).long()\n",
    "        \n",
    "        model_ij = LogisticRegressionModel(input_dim=X.shape[1])\n",
    "        model_ij = train_logistic_model(model_ij, X_ij, y_ij_binary, lr=lr, epochs=epochs, print_every=100)\n",
    "        pairwise_models[(i,j)] = model_ij\n",
    "    return pairwise_models\n",
    "\n",
    "def predict_ovo(pairwise_models, X, num_classes):\n",
    "    votes = torch.zeros((X.shape[0], num_classes), dtype=torch.int)\n",
    "    with torch.no_grad():\n",
    "        for (i, j) in pairwise_models.keys():\n",
    "            model_ij = pairwise_models[(i, j)]\n",
    "            logits_ij = model_ij(X)\n",
    "            preds_ij = (torch.sigmoid(logits_ij) >= 0.5).long()\n",
    "            votes[:, i] += (preds_ij == 0).view(-1).int()\n",
    "            votes[:, j] += (preds_ij == 1).view(-1).int()\n",
    "\n",
    "    final_preds = torch.argmax(votes, dim=1)\n",
    "    return final_preds\n",
    "\n",
    "print(\"\\n============= One-vs-One (OvO) =============\")\n",
    "ovo_models = train_ovo_classifiers(X_train, y_train, num_classes, lr=0.001, epochs=500)\n",
    "ovo_preds_test = predict_ovo(ovo_models, X_test, num_classes)\n",
    "ovo_accuracy = (ovo_preds_test == y_test).float().mean().item()\n",
    "print(f\"OvO Test Accuracy: {ovo_accuracy:.4f}\")\n",
    "\n",
    "##############################\n",
    "# 4) Confusion Matrices\n",
    "##############################\n",
    "def plot_conf_matrix(y_true, y_pred, title):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(4, 3))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "\n",
    "plot_conf_matrix(y_test, ova_preds_test, \"OvA Confusion Matrix (Test)\")\n",
    "plot_conf_matrix(y_test, ovo_preds_test, \"OvO Confusion Matrix (Test)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
